{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import torch\n",
    "from torch.jit import script, trace\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "import csv\n",
    "import random\n",
    "import re\n",
    "import os\n",
    "import unicodedata\n",
    "import codecs\n",
    "from io import open\n",
    "import itertools\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default word tokens\n",
    "PAD_token = 0  # Used for padding short sentences\n",
    "SOS_token = 1  # Start-of-sentence token\n",
    "EOS_token = 2  # End-of-sentence token\n",
    "\n",
    "class Voc:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.trimmed = False\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3  # Count SOS, EOS, PAD\n",
    "\n",
    "    def addSentence(self, sentence):\n",
    "        for word in sentence.split(' '):\n",
    "            self.addWord(word)\n",
    "\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    # Remove words below a certain count threshold\n",
    "    def trim(self, min_count):\n",
    "        if self.trimmed:\n",
    "            return\n",
    "        self.trimmed = True\n",
    "\n",
    "        keep_words = []\n",
    "\n",
    "        for k, v in self.word2count.items():\n",
    "            if v >= min_count:\n",
    "                keep_words.append(k)\n",
    "\n",
    "        print('keep_words {} / {} = {:.4f}'.format(\n",
    "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
    "        ))\n",
    "\n",
    "        # Reinitialize dictionaries\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
    "        self.num_words = 3 # Count default tokens\n",
    "\n",
    "        for word in keep_words:\n",
    "            self.addWord(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"ain't\": 'am not',\n",
       " \"amn't\": 'am not',\n",
       " \"aren't\": 'are not',\n",
       " \"can't\": 'cannot',\n",
       " \"'cause\": 'because',\n",
       " \"could've\": 'could have',\n",
       " \"couldn't\": 'could not',\n",
       " \"couldn't've\": 'could not have',\n",
       " \"daren't\": 'dare not',\n",
       " \"daresn't\": 'dare not',\n",
       " \"dasn't\": 'dare not',\n",
       " \"didn't\": 'did not',\n",
       " \"doesn't\": 'does not',\n",
       " \"don't\": 'do not',\n",
       " \"e'er\": 'ever',\n",
       " \"everyone's\": 'everyone is',\n",
       " 'finna': 'fixing to',\n",
       " 'gimme': 'give me',\n",
       " 'gonna': 'going to',\n",
       " \"gon't\": 'go not',\n",
       " 'gotta': 'got to',\n",
       " \"hadn't\": 'had not',\n",
       " \"hasn't\": 'has not',\n",
       " \"haven't\": 'have not',\n",
       " \"he'd\": 'he had',\n",
       " \"he'll\": 'he shall',\n",
       " \"he's\": 'he has',\n",
       " \"he've\": 'he have',\n",
       " \"how'd\": 'how did',\n",
       " \"how'll\": 'how will',\n",
       " \"how're\": 'how are',\n",
       " \"how's\": 'how has',\n",
       " \"i'd\": 'i had',\n",
       " \"i'll\": 'i shall',\n",
       " \"i'm\": 'i am',\n",
       " \"i'm'a\": 'i am about to',\n",
       " \"i'm'o\": 'i am going to',\n",
       " \"i've\": 'i have',\n",
       " \"isn't\": 'is not',\n",
       " \"it'd\": 'it would',\n",
       " \"it'll\": 'it shall',\n",
       " \"it's\": 'it has',\n",
       " \"let's\": 'let us',\n",
       " \"mayn't\": 'may not',\n",
       " \"may've\": 'may have',\n",
       " \"mightn't\": 'might not',\n",
       " \"might've\": 'might have',\n",
       " \"mustn't\": 'must not',\n",
       " \"mustn't've\": 'must not have',\n",
       " \"must've\": 'must have',\n",
       " \"needn't\": 'need not',\n",
       " \"ne'er\": 'never',\n",
       " \"o'clock\": 'of the clock',\n",
       " \"o'er\": 'over',\n",
       " \"ol'\": 'old',\n",
       " \"oughtn't\": 'ought not',\n",
       " \"'s\": 'is has does or us',\n",
       " \"shalln't\": 'shall not (archaic)',\n",
       " \"shan't\": 'shall not',\n",
       " \"she'd\": 'she had',\n",
       " \"she'll\": 'she shall',\n",
       " \"she's\": 'she has',\n",
       " \"should've\": 'should have',\n",
       " \"shouldn't\": 'should not',\n",
       " \"shouldn't've\": 'should not have',\n",
       " \"somebody's\": 'somebody has',\n",
       " \"someone's\": 'someone has',\n",
       " \"something's\": 'something has',\n",
       " \"so're\": 'so are',\n",
       " \"that'll\": 'that shall',\n",
       " \"that're\": 'that are',\n",
       " \"that's\": 'that has',\n",
       " \"that'd\": 'that would',\n",
       " \"there'd\": 'there had',\n",
       " \"there'll\": 'there shall',\n",
       " \"there're\": 'there are',\n",
       " \"there's\": 'there has',\n",
       " \"these're\": 'these are',\n",
       " \"they'd\": 'they had',\n",
       " \"they'll\": 'they shall',\n",
       " \"they're\": 'they are',\n",
       " \"they've\": 'they have',\n",
       " \"this's\": 'this has',\n",
       " \"those're\": 'those are',\n",
       " \"'tis\": 'it is',\n",
       " \"'twas\": 'it was',\n",
       " \"wasn't\": 'was not',\n",
       " \"we'd\": 'we had',\n",
       " \"we'd've\": 'we would have',\n",
       " \"we'll\": 'we will',\n",
       " \"we're\": 'we are',\n",
       " \"we've\": 'we have',\n",
       " \"weren't\": 'were not',\n",
       " \"what'd\": 'what did',\n",
       " \"what'll\": 'what shall',\n",
       " \"what're\": 'what are what were',\n",
       " \"what's\": 'what has',\n",
       " \"what've\": 'what have',\n",
       " \"when's\": 'when has',\n",
       " \"where'd\": 'where did',\n",
       " \"where're\": 'where are',\n",
       " \"where's\": 'where has',\n",
       " \"where've\": 'where have',\n",
       " \"which's\": 'which has',\n",
       " \"who'd\": 'who would',\n",
       " \"who'd've\": 'who would have',\n",
       " \"who'll\": 'who shall',\n",
       " \"whom'st'd've\": 'whom hast had have',\n",
       " \"who're\": 'who are',\n",
       " \"who's\": 'who has',\n",
       " \"who've\": 'who have',\n",
       " \"why'd\": 'why did',\n",
       " \"why're\": 'why are',\n",
       " \"why's\": 'why has',\n",
       " \"won't\": 'will not',\n",
       " \"would've\": 'would have',\n",
       " \"wouldn't\": 'would not',\n",
       " \"y'all\": 'you all',\n",
       " \"you'd\": 'you had',\n",
       " \"you'll\": 'you shall',\n",
       " \"you're\": 'you are',\n",
       " \"you've\": 'you have',\n",
       " \"c'mon\": 'come on',\n",
       " \"gettin'\": 'getting',\n",
       " 'matey': 'mate',\n",
       " 'wanna': 'want to',\n",
       " 'whaddya': 'what do you'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile = os.path.join(\".\",\"final.txt\")\n",
    "MAX_LENGTH = 12  # Maximum sentence length to consider\n",
    "import json\n",
    "with open('contractions.json', 'r') as fp:\n",
    "    good_prefixes = json.load(fp)\n",
    "    good_prefixes = {key.strip().lower():value.strip().lower() for key,value in good_prefixes.items()}\n",
    "\n",
    "good_prefixes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./final.txt'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start preparing training data ...\n",
      "Reading lines...\n",
      "Read 503 sentence pairs\n",
      "Trimmed to 393 sentence pairs\n",
      "Counting words...\n",
      "Counted words: 937\n",
      "\n",
      "pairs:\n",
      "['hello', 'hello sir mam how can we assist you ?']\n",
      "['i want to know about the managed wifi .', 'managed wifiwe have come up with a feature that gives you access to the list of devices connected to your router and total bandwidth usage . furthermore with just a small upgrade you can get dual band router for better internet experience .you can also gain information about the application from the https ww .youtube .com watch ?v faxyr c jfs']\n",
      "['hello', 'hello sir mam how can we assist you ?']\n",
      "['i have to register for this feature ?', 'yes sir mam .']\n",
      "['what kind of user are eligible for this feature ?', 'all the worldlink user are eligible for this feature .']\n",
      "['how long it will take to register ?', 'it will take about one day sir .']\n",
      "['okay sir', 'welcome sir mam']\n",
      "['hi', 'hello sir mam how can we assist you ?']\n",
      "['can you describe me about this discount feature ?', 'discount appfor every rupee not spent is a rupee saved . we have introduced a discount app feature to benefit our customers . we provide privilege discounts and deals at outlets ranging from restaurants retail stores fitness centers and many more . in order to claim the discount the users will need to show their active go app to the respective merchants before the bill is printed .https play .google .com store apps details ?id np .com .worldlink .worldlinkgoapp .http godiscount .com .np client register']\n",
      "['how long it will take to register ?', 'it will take about one day sir .']\n"
     ]
    }
   ],
   "source": [
    "# Turn a Unicode string to plain ASCII, thanks to\n",
    "# https://stackoverflow.com/a/518232/2809427\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = \" \".join([good_prefixes[each] if each in good_prefixes else each for each in s.split()])\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
    "#    s = re.sub(r'\\.+', \" \", s)\n",
    "    pattern = re.compile(r\"(.)\\1{2,}\")\n",
    "    s=  pattern.sub(r\"\\1\\1\", s)\n",
    "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
    "    return s\n",
    "\n",
    "# Read query/response pairs and return a voc object\n",
    "def readVocs(datafile, corpus_name):\n",
    "    print(\"Reading lines...\")\n",
    "    # Read the file and split into lines\n",
    "    lines = open(datafile, encoding='utf-8').\\\n",
    "        read().strip().split(\"\\n\")\n",
    "    # Split every line into pairs and normalize\n",
    "#     pairs = [[normalizeString(s) for s in l.split('<CoSe>')] for l in lines]\n",
    "    human = []\n",
    "    reply = []\n",
    "    for line in lines:\n",
    "        if line.lower().startswith('human'):\n",
    "            if human != []:\n",
    "                if p_v.lower().startswith('human'):\n",
    "                    human[-1] = human[-1] + \". \" + normalizeString(line.strip().partition(\":\")[2])\n",
    "#                     human[-1] = human[-1] + \". \" + line.strip().partition(':')[2]\n",
    "                else:\n",
    "                    human.append(normalizeString(line.strip().partition(':')[2]))\n",
    "            else:\n",
    "                human.append(normalizeString(line.strip().partition(':')[2]))\n",
    "        elif line.lower().startswith('reply'):\n",
    "            if reply != []:\n",
    "                if p_v.lower().startswith('reply'):\n",
    "                    reply[-1] = reply[-1] + \". \" + normalizeString(line.strip().partition(':')[2])\n",
    "                else:\n",
    "                    reply.append(normalizeString(line.strip().partition(\":\")[2]))\n",
    "            else:\n",
    "                reply.append(normalizeString(line.strip().partition(':')[2]))\n",
    "        else:\n",
    "            reply[-1]=reply[-1]+ normalizeString(line.strip())\n",
    "        p_v = line\n",
    "        \n",
    "    pairs = []\n",
    "    for h, r in zip(human,reply):\n",
    "        pairs.append([h,r])\n",
    "        \n",
    "    voc = Voc(corpus_name)\n",
    "    return voc, pairs\n",
    "\n",
    "# Returns True iff both sentences in a pair 'p' are under the MAX_LENGTH threshold\n",
    "def filterPair(p):\n",
    "    # Input sequences need to preserve the last word for EOS token\n",
    "    if len(p)==2:\n",
    "        return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split('  ')) < MAX_LENGTH\n",
    "\n",
    "# Filter pairs using filterPair condition\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]\n",
    "\n",
    "# Using the functions defined above, return a populated voc object and pairs list\n",
    "def loadPrepareData(corpus_name, datafile, save_dir):\n",
    "    print(\"Start preparing training data ...\")\n",
    "    voc, pairs = readVocs(datafile, corpus_name)\n",
    "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
    "    pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        voc.addSentence(pair[0])\n",
    "        voc.addSentence(pair[1])\n",
    "    print(\"Counted words:\", voc.num_words)\n",
    "    return voc, pairs\n",
    "\n",
    "\n",
    "# Load/Assemble voc and pairs\n",
    "save_dir = os.path.join(\".\",\"model\", \"save\")\n",
    "corpus_name = 'chatbot'\n",
    "voc, pairs = loadPrepareData(corpus_name, datafile, save_dir)\n",
    "# Print some pairs to validate\n",
    "print(\"\\npairs:\")\n",
    "for pair in pairs[:10]:\n",
    "    print (pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MIN_COUNT = 3    # Minimum word count threshold for trimming\n",
    "\n",
    "# def trimRareWords(voc, pairs, MIN_COUNT):\n",
    "#     # Trim words used under the MIN_COUNT from the voc\n",
    "#     voc.trim(MIN_COUNT)\n",
    "#     # Filter out pairs with trimmed words\n",
    "#     keep_pairs = []\n",
    "#     for pair in pairs:\n",
    "#         input_sentence = pair[0]\n",
    "#         output_sentence = pair[1]\n",
    "#         keep_input = True\n",
    "#         keep_output = True\n",
    "#         # Check input sentence\n",
    "#         for word in input_sentence.split(' '):\n",
    "#             if word not in voc.word2index:\n",
    "#                 keep_input = False\n",
    "#                 break\n",
    "#         # Check output sentence\n",
    "#         for word in output_sentence.split(' '):\n",
    "#             if word not in voc.word2index:\n",
    "#                 keep_output = False\n",
    "#                 break\n",
    "\n",
    "#         # Only keep pairs that do not contain trimmed word(s) in their input or output sentence\n",
    "#         if keep_input and keep_output:\n",
    "#             keep_pairs.append(pair)\n",
    "\n",
    "#     print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
    "#     return keep_pairs\n",
    "\n",
    "\n",
    "# # Trim voc and pairs\n",
    "# pairs = trimRareWords(voc, pairs, MIN_COUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(\".\",\"pickles\",\"vocab.file\"), \"wb\") as f:\n",
    "#     pickle.dump(voc, f, pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(os.path.join(\".\",\"pickles\",\"vocab.file\"), \"rb\") as f:\n",
    "#     dump = pickle.load(f)\n",
    "# print(dir(dump))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_variable: tensor([[423,   7,  73, 698,  85],\n",
      "        [ 10,  10,  10,   4,   2],\n",
      "        [147, 675, 763,  20,   0],\n",
      "        [570,  87,  11,   2,   0],\n",
      "        [ 17,  14,   2,   0,   0],\n",
      "        [729, 463,   0,   0,   0],\n",
      "        [572, 154,   0,   0,   0],\n",
      "        [ 32, 695,   0,   0,   0],\n",
      "        [ 17,  20,   0,   0,   0],\n",
      "        [446,   2,   0,   0,   0],\n",
      "        [ 20,   0,   0,   0,   0],\n",
      "        [  2,   0,   0,   0,   0]])\n",
      "lengths: tensor([12, 10,  5,  4,  2])\n",
      "target_variable: tensor([[469, 167, 577, 698,   3],\n",
      "        [  4,  10,   4,   4,   4],\n",
      "        [730, 179,   5,  20,   5],\n",
      "        [731,  76,   2,   6,   6],\n",
      "        [317, 164,   0,   7,   7],\n",
      "        [732,  97,   0,   8,   8],\n",
      "        [312,  20,   0,   9,   9],\n",
      "        [ 14,   2,   0,  10,  10],\n",
      "        [733,   0,   0,  11,  11],\n",
      "        [734,   0,   0,   2,   2],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [735,   0,   0,   0,   0],\n",
      "        [312,   0,   0,   0,   0],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [733,   0,   0,   0,   0],\n",
      "        [736,   0,   0,   0,   0],\n",
      "        [737,   0,   0,   0,   0],\n",
      "        [312,   0,   0,   0,   0],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [733,   0,   0,   0,   0],\n",
      "        [734,   0,   0,   0,   0],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [735,   0,   0,   0,   0],\n",
      "        [312,   0,   0,   0,   0],\n",
      "        [ 14,   0,   0,   0,   0],\n",
      "        [733,   0,   0,   0,   0],\n",
      "        [738,   0,   0,   0,   0],\n",
      "        [  2,   0,   0,   0,   0]])\n",
      "mask: tensor([[ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True,  True,  True,  True],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True,  True, False,  True,  True],\n",
      "        [ True, False, False,  True,  True],\n",
      "        [ True, False, False,  True,  True],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False],\n",
      "        [ True, False, False, False, False]])\n",
      "max_target_len: 28\n"
     ]
    }
   ],
   "source": [
    "def indexesFromSentence(voc, sentence):\n",
    "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
    "\n",
    "\n",
    "def zeroPadding(l, fillvalue=PAD_token):\n",
    "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
    "\n",
    "def binaryMatrix(l, value=PAD_token):\n",
    "    m = []\n",
    "    for i, seq in enumerate(l):\n",
    "        m.append([])\n",
    "        for token in seq:\n",
    "            if token == PAD_token:\n",
    "                m[i].append(0)\n",
    "            else:\n",
    "                m[i].append(1)\n",
    "    return m\n",
    "\n",
    "# Returns padded input sequence tensor and lengths\n",
    "def inputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, lengths\n",
    "\n",
    "# Returns padded target sequence tensor, padding mask, and max target length\n",
    "def outputVar(l, voc):\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
    "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
    "    padList = zeroPadding(indexes_batch)\n",
    "    mask = binaryMatrix(padList)\n",
    "    mask = torch.BoolTensor(mask)\n",
    "    padVar = torch.LongTensor(padList)\n",
    "    return padVar, mask, max_target_len\n",
    "\n",
    "# Returns all items for a given batch of pairs\n",
    "def batch2TrainData(voc, pair_batch):\n",
    "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
    "    input_batch, output_batch = [], []\n",
    "    for pair in pair_batch:\n",
    "        input_batch.append(pair[0])\n",
    "        output_batch.append(pair[1])\n",
    "    inp, lengths = inputVar(input_batch, voc)\n",
    "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
    "    return inp, lengths, output, mask, max_target_len\n",
    "\n",
    "\n",
    "# Example for validation\n",
    "small_batch_size = 5\n",
    "batches = batch2TrainData(voc, [random.choice(pairs) for _ in range(small_batch_size)])\n",
    "# batches = batch2TrainData(voc,[for _ in range(small_batch_size)])\n",
    "input_variable, lengths, target_variable, mask, max_target_len = batches\n",
    "\n",
    "print(\"input_variable:\", input_variable)\n",
    "print(\"lengths:\", lengths)\n",
    "print(\"target_variable:\", target_variable)\n",
    "print(\"mask:\", mask)\n",
    "print(\"max_target_len:\", max_target_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embedding = embedding\n",
    "\n",
    "        # Initialize GRU; the input_size and hidden_size params are both set to 'hidden_size'\n",
    "        #   because our input size is a word embedding with number of features == hidden_size\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
    "                          dropout=(0 if n_layers == 1 else dropout), bidirectional=True)\n",
    "\n",
    "    def forward(self, input_seq, input_lengths, hidden=None):\n",
    "#         print (\"-------------------------\")\n",
    "#         print (\"encoder forward running\")\n",
    "        # Convert word indexes to embeddings\n",
    "        embedded = self.embedding(input_seq)\n",
    "#         print (\"encoder embeded: \", embedded)\n",
    "        # Pack padded batch of sequences for RNN module\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
    "#         print (\"encoder packed: \", packed)\n",
    "        # Forward pass through GRU\n",
    "        outputs, hidden = self.gru(packed, hidden)\n",
    "        # Unpack padding\n",
    "        outputs, _ = torch.nn.utils.rnn.pad_packed_sequence(outputs)\n",
    "        # Sum bidirectional GRU outputs\n",
    "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, : ,self.hidden_size:]\n",
    "#         print (\"encoder outputs: \", outputs)\n",
    "#         print (\"encoder hidden: \", hidden)\n",
    "        # Return output and final hidden state\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Luong attention layer\n",
    "class Attn(torch.nn.Module):\n",
    "    def __init__(self, method, hidden_size):\n",
    "        super(Attn, self).__init__()\n",
    "        self.method = method\n",
    "        if self.method not in ['dot', 'general', 'concat']:\n",
    "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
    "        self.hidden_size = hidden_size\n",
    "        if self.method == 'general':\n",
    "#             print (\"general: \", self.method)\n",
    "            self.attn = torch.nn.Linear(self.hidden_size, hidden_size)\n",
    "        elif self.method == 'concat':\n",
    "#             print (\"concat: \", self.method)\n",
    "            self.attn = torch.nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "            self.v = torch.nn.Parameter(torch.FloatTensor(hidden_size))\n",
    "\n",
    "    def dot_score(self, hidden, encoder_output):\n",
    "#         print (\"dot_score running\")\n",
    "        return torch.sum(hidden * encoder_output, dim=2)\n",
    "\n",
    "    def general_score(self, hidden, encoder_output):\n",
    "#         print (\"general_score running\")\n",
    "        energy = self.attn(encoder_output)\n",
    "        return torch.sum(hidden * energy, dim=2)\n",
    "\n",
    "    def concat_score(self, hidden, encoder_output):\n",
    "#         print (\"concat_score running\")\n",
    "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
    "        return torch.sum(self.v * energy, dim=2)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "#         print (\"forward running\")\n",
    "        # Calculate the attention weights (energies) based on the given method\n",
    "        if self.method == 'general':\n",
    "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'concat':\n",
    "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
    "        elif self.method == 'dot':\n",
    "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
    "\n",
    "        # Transpose max_length and batch_size dimensions\n",
    "        attn_energies = attn_energies.t()\n",
    "\n",
    "        # Return the softmax normalized probability scores (with added dimension)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LuongAttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
    "        super(LuongAttnDecoderRNN, self).__init__()\n",
    "\n",
    "        # Keep for reference\n",
    "        self.attn_model = attn_model\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.embedding = embedding\n",
    "        self.embedding_dropout = nn.Dropout(dropout)\n",
    "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
    "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "        self.attn = Attn(attn_model, hidden_size)\n",
    "\n",
    "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
    "        # Note: we run this one step (word) at a time\n",
    "        # Get embedding of current input word\n",
    "#         print (\"decoder forward running\")\n",
    "        embedded = self.embedding(input_step)\n",
    "#         print (\"decoder embedded: \", embedded)\n",
    "        embedded = self.embedding_dropout(embedded)\n",
    "#         print (\"decoder embedded: \", embedded)\n",
    "        # Forward through unidirectional GRU\n",
    "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
    "        # Calculate attention weights from the current GRU output\n",
    "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
    "        # Multiply attention weights to encoder outputs to get new \"weighted sum\" context vector\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
    "        # Concatenate weighted context vector and GRU output using Luong eq. 5\n",
    "        rnn_output = rnn_output.squeeze(0)\n",
    "        context = context.squeeze(1)\n",
    "        concat_input = torch.cat((rnn_output, context), 1)\n",
    "        concat_output = torch.tanh(self.concat(concat_input))\n",
    "        # Predict next word using Luong eq. 6\n",
    "        output = self.out(concat_output)\n",
    "        output = F.softmax(output, dim=1)\n",
    "#         print (\"decoder output: \", output)\n",
    "#         print (\"decoder hidden: \", hidden)\n",
    "        # Return output and final hidden state\n",
    "        return output, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def maskNLLLoss(inp, target, mask):\n",
    "    nTotal = mask.sum()\n",
    "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
    "    loss = crossEntropy.masked_select(mask).mean()\n",
    "    loss = loss.to(device)\n",
    "    return loss, nTotal.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_variable, lengths, target_variable, mask, max_target_len, encoder, decoder, embedding,\n",
    "          encoder_optimizer, decoder_optimizer, batch_size, clip, max_length=MAX_LENGTH):\n",
    "\n",
    "    # Zero gradients\n",
    "    encoder_optimizer.zero_grad()\n",
    "    decoder_optimizer.zero_grad()\n",
    "\n",
    "    # Set device options\n",
    "    input_variable = input_variable.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    target_variable = target_variable.to(device)\n",
    "    mask = mask.to(device)\n",
    "\n",
    "    # Initialize variables\n",
    "    loss = 0\n",
    "    print_losses = []\n",
    "    n_totals = 0\n",
    "\n",
    "    # Forward pass through encoder\n",
    "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
    "\n",
    "    # Create initial decoder input (start with SOS tokens for each sentence)\n",
    "    decoder_input = torch.LongTensor([[SOS_token for _ in range(batch_size)]])\n",
    "    decoder_input = decoder_input.to(device)\n",
    "\n",
    "    # Set initial decoder hidden state to the encoder's final hidden state\n",
    "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "\n",
    "    # Determine if we are using teacher forcing this iteration\n",
    "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "\n",
    "    # Forward batch of sequences through decoder one time step at a time\n",
    "    if use_teacher_forcing:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # Teacher forcing: next input is current target\n",
    "            decoder_input = target_variable[t].view(1, -1)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "    else:\n",
    "        for t in range(max_target_len):\n",
    "            decoder_output, decoder_hidden = decoder(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            # No teacher forcing: next input is decoder's own current output\n",
    "            _, topi = decoder_output.topk(1)\n",
    "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(batch_size)]])\n",
    "            decoder_input = decoder_input.to(device)\n",
    "            # Calculate and accumulate loss\n",
    "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
    "            loss += mask_loss\n",
    "            print_losses.append(mask_loss.item() * nTotal)\n",
    "            n_totals += nTotal\n",
    "\n",
    "    # Perform backpropatation\n",
    "    loss.backward()\n",
    "\n",
    "    # Clip gradients: gradients are modified in place\n",
    "    _ = torch.nn.utils.clip_grad_norm_(encoder.parameters(), clip)\n",
    "    _ = torch.nn.utils.clip_grad_norm_(decoder.parameters(), clip)\n",
    "\n",
    "    # Adjust model weights\n",
    "    encoder_optimizer.step()\n",
    "    decoder_optimizer.step()\n",
    "\n",
    "    return sum(print_losses) / n_totals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer, embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size, print_every, save_every, clip, corpus_name, loadFilename):\n",
    "\n",
    "    # Load batches for each iteration\n",
    "    training_batches = [batch2TrainData(voc, [random.choice(pairs) for _ in range(batch_size)])\n",
    "                      for _ in range(n_iteration)]\n",
    "    \n",
    "    print (\"Training Batch: \", len(training_batches))\n",
    "\n",
    "    # Initializations\n",
    "    print('Initializing ...')\n",
    "    start_iteration = 1\n",
    "    print_loss = 0\n",
    "    if loadFilename:\n",
    "        start_iteration = checkpoint['iteration'] + 1\n",
    "\n",
    "    # Training loop\n",
    "    print(\"Training...\")\n",
    "    for iteration in range(start_iteration, n_iteration + 1):\n",
    "        training_batch = training_batches[iteration - 1]\n",
    "        # Extract fields from batch\n",
    "        input_variable, lengths, target_variable, mask, max_target_len = training_batch\n",
    "\n",
    "        # Run a training iteration with batch\n",
    "        loss = train(input_variable, lengths, target_variable, mask, max_target_len, encoder,\n",
    "                     decoder, embedding, encoder_optimizer, decoder_optimizer, batch_size, clip)\n",
    "        print_loss += loss\n",
    "\n",
    "        # Print progress\n",
    "        if iteration % print_every == 0:\n",
    "            print_loss_avg = print_loss / print_every\n",
    "            train_loss.append(print_loss_avg)\n",
    "            print(\"Iteration: {}; Percent complete: {:.1f}%; Average loss: {:.4f}\".format(iteration, iteration / n_iteration * 100, print_loss_avg))\n",
    "            print_loss = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        if (iteration % save_every == 0):\n",
    "            directory = os.path.join(save_dir, model_name, corpus_name, '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size))\n",
    "            if not os.path.exists(directory):\n",
    "                os.makedirs(directory)\n",
    "            torch.save({\n",
    "                'iteration': iteration,\n",
    "                'en': encoder.state_dict(),\n",
    "                'de': decoder.state_dict(),\n",
    "                'en_opt': encoder_optimizer.state_dict(),\n",
    "                'de_opt': decoder_optimizer.state_dict(),\n",
    "                'loss': loss,\n",
    "                'voc_dict': voc.__dict__,\n",
    "                'embedding': embedding.state_dict()\n",
    "            }, os.path.join(directory, '{}_{}.tar'.format(iteration, 'checkpoint')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building encoder and decoder ...\n",
      "Models built and ready to go!\n"
     ]
    }
   ],
   "source": [
    "# Configure models\n",
    "model_name = 'cb_model'\n",
    "attn_model = 'dot'\n",
    "#attn_model = 'general'\n",
    "#attn_model = 'concat'\n",
    "hidden_size = 512\n",
    "encoder_n_layers = 2\n",
    "decoder_n_layers = 2\n",
    "dropout = 0.1\n",
    "batch_size = 64\n",
    "train_loss=[]\n",
    "\n",
    "# Set checkpoint to load from; set to None if starting from scratch\n",
    "loadFilename = None\n",
    "#checkpoint_iter = 200000\n",
    "#loadFilename = os.path.join(save_dir, model_name, corpus_name, \n",
    "#                           '{}-{}_{}'.format(encoder_n_layers, decoder_n_layers, hidden_size), \n",
    "#                             '{}_checkpoint.tar'.format(checkpoint_iter))\n",
    "\n",
    "# Load model if a loadFilename is provided\n",
    "if loadFilename:\n",
    "    # If loading on same machine the model was trained on\n",
    "    checkpoint = torch.load(loadFilename)\n",
    "    # If loading a model trained on GPU to CPU\n",
    "    #checkpoint = torch.load(loadFilename, map_location=torch.device('cpu'))\n",
    "    encoder_sd = checkpoint['en']\n",
    "    decoder_sd = checkpoint['de']\n",
    "    encoder_optimizer_sd = checkpoint['en_opt']\n",
    "    decoder_optimizer_sd = checkpoint['de_opt']\n",
    "    embedding_sd = checkpoint['embedding']\n",
    "    voc.__dict__ = checkpoint['voc_dict']\n",
    "\n",
    "\n",
    "print('Building encoder and decoder ...')\n",
    "# Initialize word embeddings\n",
    "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
    "if loadFilename:\n",
    "    embedding.load_state_dict(embedding_sd)\n",
    "# Initialize encoder & decoder models\n",
    "encoder = EncoderRNN(hidden_size, embedding, encoder_n_layers, dropout)\n",
    "decoder = LuongAttnDecoderRNN(attn_model, embedding, hidden_size, voc.num_words, decoder_n_layers, dropout)\n",
    "if loadFilename:\n",
    "    encoder.load_state_dict(encoder_sd)\n",
    "    decoder.load_state_dict(decoder_sd)\n",
    "# Use appropriate device\n",
    "encoder = encoder.to(device)\n",
    "decoder = decoder.to(device)\n",
    "print('Models built and ready to go!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "934"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(voc.word2index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Training\n",
    "~~~~~~~~~~~~\n",
    "\n",
    "Run the following block if you want to train the model.\n",
    "\n",
    "First we set training parameters, then we initialize our optimizers, and\n",
    "finally we call the ``trainIters`` function to run our training\n",
    "iterations.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building optimizers ...\n",
      "Starting Training!\n",
      "Training Batch:  500\n",
      "Initializing ...\n",
      "Training...\n",
      "Iteration: 10; Percent complete: 2.0%; Average loss: 6.4213\n",
      "Iteration: 20; Percent complete: 4.0%; Average loss: 5.5786\n",
      "Iteration: 30; Percent complete: 6.0%; Average loss: 5.1490\n",
      "Iteration: 40; Percent complete: 8.0%; Average loss: 4.9550\n",
      "Iteration: 50; Percent complete: 10.0%; Average loss: 4.4951\n",
      "Iteration: 60; Percent complete: 12.0%; Average loss: 4.0658\n",
      "Iteration: 70; Percent complete: 14.0%; Average loss: 3.4799\n",
      "Iteration: 80; Percent complete: 16.0%; Average loss: 3.1498\n",
      "Iteration: 90; Percent complete: 18.0%; Average loss: 2.6992\n",
      "Iteration: 100; Percent complete: 20.0%; Average loss: 2.6337\n",
      "Iteration: 110; Percent complete: 22.0%; Average loss: 2.4290\n",
      "Iteration: 120; Percent complete: 24.0%; Average loss: 2.1508\n",
      "Iteration: 130; Percent complete: 26.0%; Average loss: 1.7530\n",
      "Iteration: 140; Percent complete: 28.0%; Average loss: 1.7772\n",
      "Iteration: 150; Percent complete: 30.0%; Average loss: 1.6724\n",
      "Iteration: 160; Percent complete: 32.0%; Average loss: 1.4996\n",
      "Iteration: 170; Percent complete: 34.0%; Average loss: 1.3537\n",
      "Iteration: 180; Percent complete: 36.0%; Average loss: 1.0906\n",
      "Iteration: 190; Percent complete: 38.0%; Average loss: 1.0041\n",
      "Iteration: 200; Percent complete: 40.0%; Average loss: 0.8994\n",
      "Iteration: 210; Percent complete: 42.0%; Average loss: 0.8351\n",
      "Iteration: 220; Percent complete: 44.0%; Average loss: 0.7421\n",
      "Iteration: 230; Percent complete: 46.0%; Average loss: 0.6895\n",
      "Iteration: 240; Percent complete: 48.0%; Average loss: 0.6475\n",
      "Iteration: 250; Percent complete: 50.0%; Average loss: 0.6469\n",
      "Iteration: 260; Percent complete: 52.0%; Average loss: 0.5534\n",
      "Iteration: 270; Percent complete: 54.0%; Average loss: 0.4697\n",
      "Iteration: 280; Percent complete: 56.0%; Average loss: 0.5162\n",
      "Iteration: 290; Percent complete: 58.0%; Average loss: 0.4601\n",
      "Iteration: 300; Percent complete: 60.0%; Average loss: 0.3928\n",
      "Iteration: 310; Percent complete: 62.0%; Average loss: 0.3657\n",
      "Iteration: 320; Percent complete: 64.0%; Average loss: 0.3592\n",
      "Iteration: 330; Percent complete: 66.0%; Average loss: 0.3175\n",
      "Iteration: 340; Percent complete: 68.0%; Average loss: 0.3409\n",
      "Iteration: 350; Percent complete: 70.0%; Average loss: 0.2887\n",
      "Iteration: 360; Percent complete: 72.0%; Average loss: 0.2843\n",
      "Iteration: 370; Percent complete: 74.0%; Average loss: 0.2077\n",
      "Iteration: 380; Percent complete: 76.0%; Average loss: 0.2272\n",
      "Iteration: 390; Percent complete: 78.0%; Average loss: 0.2206\n",
      "Iteration: 400; Percent complete: 80.0%; Average loss: 0.2055\n",
      "Iteration: 410; Percent complete: 82.0%; Average loss: 0.1888\n",
      "Iteration: 420; Percent complete: 84.0%; Average loss: 0.1647\n",
      "Iteration: 430; Percent complete: 86.0%; Average loss: 0.1911\n",
      "Iteration: 440; Percent complete: 88.0%; Average loss: 0.1786\n",
      "Iteration: 450; Percent complete: 90.0%; Average loss: 0.1640\n",
      "Iteration: 460; Percent complete: 92.0%; Average loss: 0.1592\n",
      "Iteration: 470; Percent complete: 94.0%; Average loss: 0.1396\n",
      "Iteration: 480; Percent complete: 96.0%; Average loss: 0.1335\n",
      "Iteration: 490; Percent complete: 98.0%; Average loss: 0.1250\n",
      "Iteration: 500; Percent complete: 100.0%; Average loss: 0.1256\n"
     ]
    }
   ],
   "source": [
    "# Configure training/optimization\n",
    "clip = 50.0\n",
    "teacher_forcing_ratio = 1.0\n",
    "learning_rate = 0.0001\n",
    "decoder_learning_ratio = 5.0\n",
    "n_iteration = 500\n",
    "print_every = 10\n",
    "save_every = 25\n",
    "\n",
    "# Ensure dropout layers are in train mode\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "# Initialize optimizers\n",
    "print('Building optimizers ...')\n",
    "encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate * decoder_learning_ratio)\n",
    "if loadFilename:\n",
    "    encoder_optimizer.load_state_dict(encoder_optimizer_sd)\n",
    "    decoder_optimizer.load_state_dict(decoder_optimizer_sd)\n",
    "\n",
    "# Run training iterations\n",
    "print(\"Starting Training!\")\n",
    "trainIters(model_name, voc, pairs, encoder, decoder, encoder_optimizer, decoder_optimizer,\n",
    "           embedding, encoder_n_layers, decoder_n_layers, save_dir, n_iteration, batch_size,\n",
    "           print_every, save_every, clip, corpus_name, loadFilename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_loss)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run Evaluation\n",
    "~~~~~~~~~~~~~~\n",
    "\n",
    "To chat with your model, run the following block.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedySearchDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(GreedySearchDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, input_seq, input_length, max_length):\n",
    "        # Forward input through encoder model\n",
    "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
    "        # Prepare encoder's final hidden layer to be first hidden input to the decoder\n",
    "        decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
    "        # Initialize decoder input with SOS_token\n",
    "        decoder_input = torch.ones(1, 1, device=device, dtype=torch.long) * SOS_token\n",
    "        # Initialize tensors to append decoded words to\n",
    "        all_tokens = torch.zeros([0], device=device, dtype=torch.long)\n",
    "        all_scores = torch.zeros([0], device=device)\n",
    "        # Iteratively decode one word token at a time\n",
    "        if not USE_MULTINOMIAL:\n",
    "            for _ in range(max_length):\n",
    "                # Forward pass through decoder\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                # Obtain most likely word token and its softmax score\n",
    "                decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
    "                # Record token and score\n",
    "                all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "                all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "                # Prepare current token to be next decoder input (add a dimension)\n",
    "                decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "            # Return collections of word tokens and scores\n",
    "            return all_tokens, all_scores\n",
    "        else:\n",
    "            for _ in range(max_length):\n",
    "                # Forward pass through decoder\n",
    "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
    "                \n",
    "                # Sample from the network as a multinomial distribution\n",
    "                decoder_output_multi = decoder_output.data.view(-1).div(TEMP).exp()\n",
    "                decoder_input = torch.multinomial(decoder_output_multi, 1)\n",
    "                decoder_scores,_ = torch.max(decoder_output, dim=1)\n",
    "                # Record token and score\n",
    "                all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
    "                all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
    "                # Prepare current token to be next decoder input (add a dimension)\n",
    "                decoder_input = torch.unsqueeze(decoder_input, 0)\n",
    "            # Return collections of word tokens and scores\n",
    "            return all_tokens, all_scores\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH, temperature = False):\n",
    "    ### Format input sentence as a batch\n",
    "    # words -> indexes\n",
    "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
    "    # Create lengths tensor\n",
    "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
    "    # Transpose dimensions of batch to match models' expectations\n",
    "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
    "    # Use appropriate device\n",
    "    input_batch = input_batch.to(device)\n",
    "    lengths = lengths.to(device)\n",
    "    # Decode sentence with searcher\n",
    "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
    "    # indexes -> words\n",
    "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
    "    return decoded_words\n",
    "\n",
    "\n",
    "def evaluateInput(encoder, decoder, searcher, voc):\n",
    "    input_sentence = ''\n",
    "    while(1):\n",
    "        try:\n",
    "            # Get input sentence\n",
    "            input_sentence = input('> ')\n",
    "            # Check if it is quit case\n",
    "            if input_sentence == 'q' or input_sentence == 'quit': break\n",
    "            # Normalize sentence\n",
    "            input_sentence = normalizeString(input_sentence)\n",
    "            # Evaluate sentence\n",
    "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
    "            # Format and print response sentence\n",
    "            output_words[:] = [x for x in output_words if not (x == 'EOS' or x == 'PAD')]\n",
    "            print('Bot:', ' '.join(output_words))\n",
    "\n",
    "        except KeyError:\n",
    "            print(\"Error: Encountered unknown word.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> Namastae\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> My internet is not wokring?\n",
      "Error: Encountered unknown word.\n",
      "> my internet is not working?\n",
      "Bot: please mention your query . the channels are not available you\n",
      "> hello\n",
      "Bot: hello sir mam how can we assist you ? channels\n",
      "> fu\n",
      "Error: Encountered unknown word.\n",
      "> tero tauko\n",
      "Bot: if you continue using such offensive and vulgar words to seek our\n",
      "> Hey i am using the internet.\n",
      "Bot: please elaborate your query sir . the channels are not available\n",
      "> \n",
      "Bot: hello sir mam how can we assist you ? channels\n",
      "> i want the new connection\n",
      "Bot: we have got sir mam . you can contact our phone\n",
      "> Could you please send me the detail of new connection?\n",
      "Bot: if you continue using such offensive and vulgar words to seek our\n",
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> thank you\n",
      "Bot: welcome sir mam . please mention your query sir .\n",
      "> kada\n",
      "Error: Encountered unknown word.\n",
      "> how are you\n",
      "Bot: we are not allowed to provide our information . please mention your\n",
      "> who are you?\n",
      "Error: Encountered unknown word.\n",
      "> please call me\n",
      "Bot: if you continue using such offensive and vulgar words to seek our\n",
      "> what offensive\n",
      "Bot: please mention your query sir . how can we assist you\n",
      "> halka milyo\n",
      "Error: Encountered unknown word.\n",
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> hue\n",
      "Error: Encountered unknown word.\n",
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> plag\n",
      "Error: Encountered unknown word.\n",
      "> test\n",
      "Error: Encountered unknown word.\n",
      "> testing\n",
      "Error: Encountered unknown word.\n",
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> hi\n",
      "Bot: namastae sir mam . how can we help you ? the\n",
      "> big\n",
      "Error: Encountered unknown word.\n",
      "> djfei\n",
      "Error: Encountered unknown word.\n",
      "> kjdf\\\n",
      "Error: Encountered unknown word.\n",
      "> djfaie\n",
      "Error: Encountered unknown word.\n",
      "> kdjfa\n",
      "Error: Encountered unknown word.\n",
      "> dja\n",
      "Error: Encountered unknown word.\n",
      "> djid\n",
      "Error: Encountered unknown word.\n",
      "> retrieval\n",
      "Error: Encountered unknown word.\n",
      "> clso retrieval hybrid\n",
      "Error: Encountered unknown word.\n"
     ]
    }
   ],
   "source": [
    "USE_MULTINOMIAL = False\n",
    "TEMP = 0.7\n",
    "\n",
    "# Set dropout layers to eval mode\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "# Initialize search module\n",
    "searcher = GreedySearchDecoder(encoder, decoder)\n",
    "\n",
    "# Begin chatting (uncomment and run the following line to begin)\n",
    "evaluateInput(encoder, decoder, searcher, voc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
